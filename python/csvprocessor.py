"""
This module holds processors for data imports...

It's was implemented with a factory function, and there's some oddities here 
because which needed to be an iterable, mostly to prevent a refactoring of 
another module, that used CsvDictReader. So this module was created
to do something that needed to occur before the other module
was executed.
All classes should contain an `extract()` method, which is a generator.
All classes should expect to be given a DataImport or an object that contains
a method `load_data()` method.

Update for :py:class OldDataPayment
    Conceptually, Old imports are CSVs. We have three types, DataPayment,
    DataItem, TransactionDiscount. Processing classes describe the
    transformation process from CSV to Blvd Data.

    Currently we have concepts Reductions, Extractions and Loads.

    Reductions:
        TBD - OldDataItem needs to be refactored.

    Extractions:
        We pull data based from the CSV. E.g. Find SomeValuableData from the spreadsheet

    Loads:
        Based on extracted data we load those items into our database
    
    Mixins:
        :py:class OldMultiRowMixin: Runs data from every row into a OldSingleRowMixin.
        :py:class OldSingleRowMixin: Runs operations on a single row of data
"""
from operator import itemgetter
from decimal import Decimal
import datetime

import Datas
from Datas import get_model, logger


# Factory
def Data_import_reader(data_import):
    """
    Factory method, returns appropriate iterator for the Data_import
    instances. The factory is reponsible for utilizing the correct processing class
    given a particular instance.
    :param Data.models.TransactionImport transaction_import:
    :return row:
    :rtype dict:
    """
    # We should really add a table for setup clients/brands/locations with processor
    # classes, but i want to to wait untill we refactor for multiple payments
    # types
    PROCESS_FOR = {Datas.models.DataItem: OldDataItem,
                   Datas.models.DataPayment: OldDataPayment}
    processor_class = PROCESS_FOR.get(Data_import.content_type.model_class(),
                                      OldDefault)
    processor = processor_class(Data_import)
    return processor.run()


# Mixins
class OldMixin(object):
    """
    A mixin for working with DataAnalyzer Reports from Old
    """

    # could be model method.
    def loc_map(self, value):
        """
        shortcut method for accessing an instance column mapping.
        this returns the column header in the csv that correlates to the column
        header in csv file.
        :param str value: str representing the db column needed
        :return value:
        :rtype str:
        """
        return self.column_mapping.get(value)


class OldMultiRowMixin(OldMixin):
    """
    A mixin for reading Data_import files that are coming from Old Data
    reports. Loads data from import file on init, and sorts by date,
    number, name.
    :param list data: a List of dict, generated by CsvDictReader
    """

    def __init__(self, data):
        self.column_mapping = data.location.csv_import_column_mapping
        self.location = data.location
        self.data = sorted(data.load_data(),
                           key=itemgetter(self.loc_map('date'),
                                          self.loc_map('number'),
                                          self.loc_map('name')))


class OldSingleRowMixin(OldMixin):
    """
    A mixin for working with single row of data from Old
    :param dict row: a single row of Data, aka CsvDictReader
    :param func loc_map: a call back function mapping a value to a row. eg `OldMixin.loc_map`
    :param clients.models.Location location: an instance of .. py:class:: clients.models.Location
    """

    def __init__(self, row, loc_map=None, location=None):
        self.loc_map = loc_map
        self.location = location
        self.row = row


class OldDefault(OldMultiRowMixin):
    """
    A default DataAnalyzer reader, for legacy support.
    yields on every row in csv.
    """

    def extract(self):
        for i in self.data:
            yield i

    def run(self):
        return self.extract()

# Processors

class OldDataValuableData(OldSingleRowMixin):

    def load(self):
        scanner_code = self.row.get('scanner_code__id')
        if scanner_code:
            check_number = self.row.get(self.loc_map('check_number'))
            business_date = datetime.datetime.strptime(self.row.get(self.loc_map('business_date')),
                                                       '%m/%d/%Y')
            Datas.tasks.create_transaction.delay(
                scanner_code, self.location.id, check_number, business_date, 'SC')
        return self.row

    def __call__(self):
        return self.load()


class OldPaymentValuableData(OldSingleRowMixin):
    """
    Extracts ScannerCode from a single row in a Old DataAnaylzer
    Payment Report.
    """

    def check_valuabledata(self):
        SomeValuableData = get_model(app_label='someapp', model_name='SomeValuableData')
        try:
            candidate = self.row.get(self.loc_map('reference'))
            if len(candidate) == 20:
                svd = SomeValuableData.objects.get(a_field=candidate)
                some_key = {'some_data__id': svd.id,
                               'some_prop': 'LL'}
                self.row.update(scanner_key)

        except TypeError:
            return None
        except ScannerCode.DoesNotExist:
            logger.info("LOGGING THINGS")
            return None

    def extract(self):
        self.check_scanercode()
        return self.row

    def __call__(self):
        return self.extract()

# Readers

class OldDataItem(OldMultiRowMixin):
    """
    This class extracts data from a DataImport that is of the TransactionItem content.
    Extraction reduces duplicated data provided in the reports.
    :param object data: an object with an attribute `load_data()` which is a returns a list
    """

    def __init__(self, data):
        # we need the data sorted, we need to be sorted s.t.
        # the check numbers and items purchases are together
        # we know this wont be in true order since these are all strings, but
        # we just need them to be grouped, not a true sort.
        super(OldDataItem, self).__init__(data)
        self.column_mapping = data.location.csv_import_column_mapping
        self._Data_item = []
        self._processed = []

    def apply_reduction(self):
        """
        We reduce the instance self._Data_item to a single item,
        combining a total value
        :return: self._Data_item
        :rtype: list
        """
        num_items = len(self._Data_item)
        if num_items > 1:
            value = sum([Decimal(ti[self.loc_map('amount')])
                         for ti in self._Data_item])
        else:
            # there's only one element, no need to continue
            return self._Data_item
        if value:
            combined_Data = self._data_item.pop(0)
            combined_Data[self.loc_map('amount')] = str(value)
            self._Data_item = [combined_data]
        else:
            # this is a void, set to empty.
            self._Data_item = []
        return self._Data_item

    def is_eq(self, obj):
        """
        Compares the current obj to the current item based on the unique_together attributes
        :param dict obj: obj representing an unimported DataItem
        :returns boolean:
        :rtype bool:
        """
        # possible to pull this from the DataItem Meta class?
        unique_together = ('date', 'name', 'number')
        return all([self._current_item[self.loc_map(i)] == obj[self.loc_map(i)]
                    for i in unique_together])

    def extract(self):
        """
        Given a DataImport, we reduce and extract data for the given
        import. Yields on every item within the DataImport
        :return data_item:
        :rtype dict:
        """

        for current_item in self.data:
            # skip our already processed items
            if current_item in self._processed:
                continue

            self._current_item = current_item
            self._Data_item = filter(self.is_eq, self.data)
            # add to our processed list
            self._processed.extend(list(self._Data_item))
            # sum up values
            items = self.apply_reduction()
            # make sure there's only one item left.

            if len(items) == 1:
                yield self._Data_item[0]

    def run(self):
        return self.extract()


class OldDataPayment(OldMultiRowMixin):
    EXTRACT_PROCESS_CLASSES = (OldPaymentValuableData,)
    LOAD_PROCESS_CLASSES = (OldDataValuableData,)

    @property
    def processors(self):
        """
        Constructs a new iterable
        :return: iterable of processor classes
        :rtype: iter
        """
        return iter(
            self.EXTRACT_PROCESS_CLASSES + self.LOAD_PROCESS_CLASSES)

    def __init__(self, data):
        super(OldDataPayment, self).__init__(data)

    def process(self):
        """
        runs current row through each of a our processing classes
        """
        working_row = self.row.copy()
        for processor in self.processors:
            working_row = processor(
                working_row, location=self.location, loc_map=self.loc_map)()
        return working_row

    def run(self):
        for row in self.data:
            self.row = row
            yield self.process()
